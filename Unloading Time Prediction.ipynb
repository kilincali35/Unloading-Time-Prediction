{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is about a project, aiming towards predicting the unloading time at a customer location. Main objective was to find a regression formula, whether linear or polynomial, in order to support optimization algorithm with a better parameter estimation. We used to use a fixed unloading time before this proect. We decided to investigate if there may be a dynamic alternative to this fixed parameter. Unfortunately, data collection in this part of our company is a little bit dirty and without a clear procedure. This led to bad performance for regression algorithm in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns',40)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "import more_itertools as mi\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import random\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import anderson, normaltest\n",
    "from sklearn.exceptions import DataConversionWarning, ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(230)\n",
    "np.random.seed(230)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "text_file2 = open(r'C:\\Users\\ali.kilinc\\Desktop\\Nike Süre Tahminleme\\FinalizeOut.txt',\"w\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "data = pd.read_excel(r'C:\\Users\\ali.kilinc\\Desktop\\Nike Süre Tahminleme\\Nike_Input.xlsx', index_col= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is for initial filtering of unrealistic values, filtering non-rare unloading locations, and some drop of unnecessary columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               DESI        KOLI   AVM  UNLOAD_TIME  ARRIVAL_HOUR\n",
      "count   5748.000000  5748.00000  5748  5748.000000   5748.000000\n",
      "unique          NaN         NaN     2          NaN           NaN\n",
      "top             NaN         NaN   AVM          NaN           NaN\n",
      "freq            NaN         NaN  3605          NaN           NaN\n",
      "mean     403.678873    16.41110   NaN   730.981907     10.924322\n",
      "std      636.714177    24.07784   NaN   847.033316      2.108179\n",
      "min        3.000000     1.00000   NaN    60.000000      2.000000\n",
      "25%       54.000000     3.00000   NaN    89.000000      9.000000\n",
      "50%      144.998500     6.00000   NaN   348.000000     10.000000\n",
      "75%      465.002500    20.00000   NaN  1183.000000     12.000000\n",
      "max     6708.000000   258.00000   NaN  3598.000000     23.000000\n",
      "Index(['DESI', 'KOLI', 'AVM', 'UNLOAD_TIME', 'ARRIVAL_HOUR'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5748 entries, 14693 to 24686\n",
      "Data columns (total 5 columns):\n",
      "DESI            5748 non-null float64\n",
      "KOLI            5748 non-null int64\n",
      "AVM             5748 non-null object\n",
      "UNLOAD_TIME     5748 non-null int64\n",
      "ARRIVAL_HOUR    5748 non-null int64\n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 269.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = data[data['UNLOAD_TIME'] < 3600]\n",
    "data = data[data['TIME_PER_DESI'] < 24]\n",
    "data = data[data['TIME_PER_DESI'] > 0.5]\n",
    "data = data[data['TIME_PER_KOLI'] < 600]\n",
    "data = data[data['TIME_PER_KOLI'] > 12.5]\n",
    "\n",
    "#data = data[~data['VH_TYPE'].isin(['KAMYON', 'TIR'])]\n",
    "\n",
    "data['DATE_ARRIVAL'] = pd.to_datetime(data['DATE_ARRIVAL'], format = '%d/%m/%Y %H:%M:%S')\n",
    "data['YEAR'] = data['DATE_ARRIVAL'].dt.year.astype(str)\n",
    "data['MONTH'] = data['DATE_ARRIVAL'].dt.month.astype(str)\n",
    "\n",
    "data = data[data['YEAR'] >= '2019']\n",
    "#data = data[data['MONTH'] >= '7']\n",
    "\n",
    "data = data[~data['UNL_CC'].isin(list(data['UNL_CC'][data.groupby('UNL_CC')['UNL_CC'].transform('size') < 20]))]\n",
    "\n",
    "dropped = data[['POSITION_ID', 'DATE_ARRIVAL', 'YEAR', 'MONTH', 'DATE_UNLOADING_END', 'NAME', 'TIME_PER_DESI', 'TIME_PER_KOLI', \n",
    "                 'ARRIVAL_MONTH', 'ARRIVAL_QUARTER', 'UNL_CC', 'UNLOADING_CITY', 'VH_TYPE','ARRIVAL_DAY']]\n",
    "data.drop(columns = ['POSITION_ID', 'DATE_ARRIVAL', 'YEAR', 'MONTH', 'DATE_UNLOADING_END', 'NAME', 'TIME_PER_DESI', 'TIME_PER_KOLI', \n",
    "                      'ARRIVAL_MONTH', 'ARRIVAL_QUARTER', 'UNL_CC', 'UNLOADING_CITY', 'VH_TYPE', 'ARRIVAL_DAY'], inplace = True)\n",
    "print(data.describe(include='all'))\n",
    "print(data.columns)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5748 entries, 14693 to 24686\n",
      "Data columns (total 5 columns):\n",
      "DESI            5748 non-null float64\n",
      "KOLI            5748 non-null int64\n",
      "AVM             5748 non-null category\n",
      "UNLOAD_TIME     5748 non-null int64\n",
      "ARRIVAL_HOUR    5748 non-null category\n",
      "dtypes: category(2), float64(1), int64(2)\n",
      "memory usage: 191.3 KB\n",
      "None\n",
      "9      1438\n",
      "10     1153\n",
      "11     1069\n",
      "12      565\n",
      "13      429\n",
      "14      361\n",
      "8       270\n",
      "15      199\n",
      "16      168\n",
      "oth      96\n",
      "Name: ARRIVAL_HOUR, dtype: int64\n",
      "AVM     3605\n",
      "NAVM    2143\n",
      "Name: AVM, dtype: int64\n",
      "['oth']\n"
     ]
    }
   ],
   "source": [
    "#data['VH_TYPE'] = data['VH_TYPE'].astype('category')\n",
    "#data['UNLOADING_CITY'] = data['UNLOADING_CITY'].astype('category')\n",
    "data['ARRIVAL_HOUR'] = data['ARRIVAL_HOUR'].astype('category')\n",
    "data['AVM'] = data['AVM'].astype('category')\n",
    "#data['UNL_CC'] = data['UNL_CC'].astype('category')\n",
    "print(data.info())\n",
    "\n",
    "transform = 1\n",
    "regress = 1\n",
    "finalize = 1\n",
    "\n",
    "#data['UNL_CC'].replace(sorted(set(data['UNL_CC'][data.groupby('UNL_CC')['UNL_CC'].transform('size') < 150])) , 'othcc', inplace = True)\n",
    "#data['UNL_CC'] = data['UNL_CC'].astype('str')\n",
    "\n",
    "#print(data['VH_TYPE'].value_counts())\n",
    "#print(data['UNLOADING_CITY'].value_counts())\n",
    "print(data['ARRIVAL_HOUR'].value_counts())\n",
    "#print(data['ARRIVAL_DAY'].value_counts())\n",
    "#print(data['ARRIVAL_MONTH'].value_counts())\n",
    "#print(data['ARRIVAL_QUARTER'].value_counts())\n",
    "print(data['AVM'].value_counts())\n",
    "#print(data['UNL_CC'].value_counts())\n",
    "\n",
    "#data = data[data['UNL_CC'] != 'othcc']\n",
    "\n",
    "threshold = 125\n",
    "\n",
    "print(sorted(set(data['ARRIVAL_HOUR'][data.groupby('ARRIVAL_HOUR')['ARRIVAL_HOUR'].transform('size') < threshold])))\n",
    "\n",
    "data['ARRIVAL_HOUR'].replace(sorted(set(data['ARRIVAL_HOUR'][data.groupby('ARRIVAL_HOUR')['ARRIVAL_HOUR'].transform('size') < threshold])) , 'oth', inplace = True)\n",
    "data['ARRIVAL_HOUR'] = data['ARRIVAL_HOUR'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code tries to filter rare unloading hours from data, mostly the ones out-of-office-hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5748 entries, 14693 to 24686\n",
      "Data columns (total 3 columns):\n",
      "DESI            5748 non-null float64\n",
      "AVM             5748 non-null category\n",
      "ARRIVAL_HOUR    5748 non-null object\n",
      "dtypes: category(1), float64(1), object(1)\n",
      "memory usage: 140.4+ KB\n",
      "None\n",
      "['AVM', 'ARRIVAL_HOUR']\n",
      "['DESI']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsns.pairplot(df_x)\\n\\nvif = [variance_inflation_factor(df_x[num_cols].values, i) for i in range(df_x[num_cols].shape[1])]\\nprint(vif)\\n\\nsns.heatmap(df_x[num_cols].corr(), annot = True)\\nplt.show()\\n\\nsns.distplot(df_y.values.ravel(),color=\"g\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#!!!!!!!! IT IS THE TRANSFORMATION OF CYCLICAL VARIABLES LIKE MONTH AND QUARTER. ONE-HOT IS AN OPTION BUT IT IS MUCH MORE BETTER THAN ENCODING\n",
    "df_x['month_x'] = np.sin(2*np.pi*df_x['ARRIVAL_MONTH']/12)\n",
    "df_x['month_y'] = np.cos(2*np.pi*df_x['ARRIVAL_MONTH']/12)\n",
    "\n",
    "df_x['quarter_x'] = np.sin(2*np.pi*df_x['ARRIVAL_QUARTER']/4)\n",
    "df_x['quarter_y'] = np.sin(2-np.pi*df_x['ARRIVAL_QUARTER]/4)\n",
    "\n",
    "df_x['day_x'] = np.sin(2*np.pi*df_x['ARRIVAL_DAY']/7)\n",
    "df_x['day_y'] = np.cos(2*np.pi*df_x['ARRIVAL_DAY']/7)\n",
    "\n",
    "df_x.drop(columns = ['ARRIVAL_MONTH', 'ARRIVAL_QUARTER', 'ARRIVAL_DAY'], axis = 1, inplace = True)\n",
    "\"\"\"\n",
    "\"\"\" # General function for this transformation\n",
    "def encode(data, col, max_val): # max val is theoretical maximum of column, for month it is 12, for day of year it is 365 etc.\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "\"\"\"\n",
    "\n",
    "#data['ARRIVAL_DAY'] = data['ARRIVAL_DAY'].astype('str')\n",
    "#data['ARRIVAL_MONTH'] = data['ARRIVAL_MONTH'].astype('str')\n",
    "#data['ARRIVAL_QUARTER'] = data['ARRIVAL_QUARTER'].astype('str')\n",
    "\n",
    "df_y = data['UNLOAD_TIME']\n",
    "df_x = data.drop(columns = ['UNLOAD_TIME'], axis=1)\n",
    "\n",
    "fix_cols = []\n",
    "for col in df_x.columns:\n",
    "    if df_x[col].nunique() == 1:\n",
    "        fix_cols.append(col)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "emp_cols = []\n",
    "for col in df_x.columns:\n",
    "    if df_x[col].count() > 0:\n",
    "        pass\n",
    "    else:\n",
    "        emp_cols.append(col)\n",
    "\n",
    "df_x.drop(columns = list(emp_cols+fix_cols), inplace = True)\n",
    "\n",
    "df_x.drop(columns = ['KOLI'], inplace = True)\n",
    "num_cols = list(df_x._get_numeric_data().columns)\n",
    "\n",
    "print(df_x.info())\n",
    "\n",
    "cat_cols = list(set(df_x.columns) - set(num_cols) - set(emp_cols) - set(fix_cols))\n",
    "num_cols = list(set(num_cols) - set(fix_cols) - set(emp_cols))\n",
    "\n",
    "print(cat_cols)\n",
    "print(num_cols)\n",
    "\n",
    "df_x = pd.DataFrame(df_x, columns = df_x.columns)\n",
    "df_y = pd.DataFrame(df_y, columns = ['UNLOAD_TIME'])\n",
    "\n",
    "\"\"\"\n",
    "sns.pairplot(df_x)\n",
    "\n",
    "vif = [variance_inflation_factor(df_x[num_cols].values, i) for i in range(df_x[num_cols].shape[1])]\n",
    "print(vif)\n",
    "\n",
    "sns.heatmap(df_x[num_cols].corr(), annot = True)\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(df_y.values.ravel(),color=\"g\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5748 entries, 21302 to 22694\n",
      "Data columns (total 3 columns):\n",
      "DESI            5748 non-null float64\n",
      "AVM             5748 non-null category\n",
      "ARRIVAL_HOUR    5748 non-null object\n",
      "dtypes: category(1), float64(1), object(1)\n",
      "memory usage: 140.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5748 entries, 21302 to 22694\n",
      "Data columns (total 1 columns):\n",
      "UNLOAD_TIME    5748 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 89.8 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#data shuffler\n",
    "idx = np.random.permutation(df_x.index)\n",
    "df_x = df_x.reindex(idx)\n",
    "df_y = df_y.reindex(idx)\n",
    "\n",
    "print(df_x.info())\n",
    "print(df_y.info())\n",
    "\n",
    "#print(df_x['ARRIVAL_HOUR'].value_counts())\n",
    "\n",
    "#y is a lognormal distribution with mostly 0 values, i will try to fit a log(x+1) transformation for that.( same for all month variables)\n",
    "#log1p(x) is equal to log(x+1). reverse of it is expm1(y)\n",
    "\n",
    "\"\"\"\n",
    "#for normality test\n",
    "result = anderson(df_y.values.ravel())\n",
    "print('Statistic: %.3f' % result.statistic)\n",
    "p = 0\n",
    "for i in range(len(result.critical_values)):\n",
    "\tsl, cv = result.significance_level[i], result.critical_values[i]\n",
    "\tif result.statistic < result.critical_values[i]:\n",
    "\t\tprint('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
    "\telse:\n",
    "\t\tprint('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))\n",
    "\n",
    "stat, p1 = normaltest(df_y.values.ravel())\n",
    "print('Statistics=%.3f, p1=%.3f' % (stat, p1))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Sample does not look Gaussian (reject H0)')\n",
    "\"\"\"\n",
    "\n",
    "if transform == 1:\n",
    "    #sns.distplot(df_y, hist = False, kde = True)\n",
    "    #plt.show()\n",
    "    #df_y = np.log1p(df_y)\n",
    "    df_y = np.log(df_y)\n",
    "    #df_y = pow(df_y, 0.5)\n",
    "    #df_y = np.log(df_y)\n",
    "    #sns.distplot(df_y, hist = True, kde = True)\n",
    "    #plt.show()\n",
    "    #df_x[month_var] = np.log(df_x[month_var]+1)\n",
    "    #pred_set[month_var] = np.log(pred_set[month_var]+1)\n",
    "else:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code is for shuffling data, then there is a section for normality test. And also an optional transformation section for dependent variable if it is skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, MinMaxScaler, Normalizer, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, cohen_kappa_score\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.metrics as m\n",
    "from lightgbm import LGBMClassifier\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Lasso, Ridge, ElasticNet, HuberRegressor, Lars, RANSACRegressor, PassiveAggressiveRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AVM', 'NAVM']\n",
      "['16', '8', '11', '10', '9', 'oth', '14', '12', '13', '15']\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = lr\n",
      "train_mse = -0.6947036147253349, test_mse =-0.7320868529243543\n",
      "train_mae = -0.6845138787357334, test_mae =-0.6959246215002357\n",
      "train_r2 = 0.5882822509973737, test_r2 = 0.5656986556447459\n",
      "avg_fit_time = 0.26725354194641116\n",
      "best_params = {}\n",
      "---0.0 minutes---\n",
      "...9628.649251747634...\n",
      "current_time = 2020-07-16 14:49:15.289688...\n",
      "current_time = 2020-07-16 14:49:15.289688...\n",
      "-----------------------------------\n",
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=-1)]: Done 239 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 680 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 840 out of 840 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = las\n",
      "train_mse = -0.7101637807010525, test_mse =-0.7210167864105241\n",
      "train_mae = -0.6949889442435507, test_mae =-0.7003904965086367\n",
      "train_r2 = 0.5791188997754786, test_r2 = 0.5721936167329195\n",
      "avg_fit_time = 0.27086315155029295\n",
      "best_params = {'est__alpha': 0.0001, 'est__max_iter': 500.0, 'est__tol': 0.1}\n",
      "---1.8 minutes---\n",
      "...9629.325979690671...\n",
      "current_time = 2020-07-16 14:51:05.473158...\n",
      "current_time = 2020-07-16 14:51:05.473158...\n",
      "-----------------------------------\n",
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   12.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = rid\n",
      "train_mse = -0.7021515376291443, test_mse =-0.7207235475026955\n",
      "train_mae = -0.6892818135719423, test_mae =-0.6972499592852454\n",
      "train_r2 = 0.5838695838506023, test_r2 = 0.572418503111378\n",
      "avg_fit_time = 0.0953444480895996\n",
      "best_params = {'est__alpha': 0.01, 'est__max_iter': 0.0, 'est__tol': 0.001}\n",
      "---0.4 minutes---\n",
      "...9739.501984188417...\n",
      "current_time = 2020-07-16 14:51:31.177046...\n",
      "current_time = 2020-07-16 14:51:31.177046...\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 840 out of 840 | elapsed:   25.4s finished\n"
     ]
    }
   ],
   "source": [
    "if regress == 1:\n",
    "\n",
    "    models = dict()\n",
    "    \n",
    "    paramslr = {\n",
    "                }\n",
    "\n",
    "    paramslas = {\n",
    "            'est__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'est__max_iter': np.linspace(0, 3000, 7, endpoint = True),\n",
    "            'est__tol': [0.0001, 0.001, 0.01, 0.1]\n",
    "                }\n",
    "    \n",
    "    paramsrid = {\n",
    "            'est__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'est__max_iter': np.linspace(0, 3000, 7, endpoint = True),\n",
    "            'est__tol': [0.0001, 0.001, 0.01, 0.1]\n",
    "                }\n",
    "    \n",
    "    paramsela = {\n",
    "            'est__alpha': [0.001, 0.01, 0.1, 1],\n",
    "            'est__max_iter': np.linspace(0, 3000, 3, endpoint = True),\n",
    "            'est__tol': [0.0001, 0.001, 0.01],\n",
    "            'est__l1_ratio': np.linspace(0.25,0.75, 3, endpoint = True)\n",
    "            }\n",
    "    \n",
    "    paramslgb = {\n",
    "            'est__max_depth':[5, 9, 18, 32],\n",
    "            'est__learning_rate': [0.001, 0.01, 0.1],\n",
    "            'est__n_estimators': [10, 50, 100, 200],\n",
    "            'est__num_leaves': np.linspace(11,51,3,endpoint = True, dtype = int)\n",
    "            }\n",
    "    \n",
    "    models['lr'] = [LinearRegression(), paramslr]\n",
    "    models['las'] = [Lasso(), paramslas]\n",
    "    models['rid'] = [Ridge(), paramsrid]\n",
    "    #models['ela'] = [ElasticNet(), paramsela]\n",
    "    #models['lgb'] = [LGBMRegressor(), paramslgb]\n",
    "    \n",
    "    for i in cat_cols:\n",
    "        print(list(mi.unique_everseen(df_x[i])))\n",
    "    \n",
    "    for key, value in models.items():\n",
    "        \n",
    "        #this part is for only gridsearchcv. We can use CV results, as well as best parameters of the grid search from here. \n",
    "    \n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        scorer = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "        \n",
    "        preprocessor = ColumnTransformer(transformers = [('num', MinMaxScaler(feature_range = (0,1)), num_cols), \n",
    "                                                         ('cat', OneHotEncoder(sparse = True, handle_unknown = 'ignore'), cat_cols)])\n",
    "        \n",
    "        poly = PolynomialFeatures(3)\n",
    "        \n",
    "        all_pipe = Pipeline(steps = [('prep', preprocessor),('poly', poly), ('est', value[0])])\n",
    "        \n",
    "        search_space = value[1]\n",
    "                \n",
    "        #njobs = -1 uses 2 cores (all available) and 4 threads; consumes 100% of CPU. But it is faster absolutely. Njobs = 1 uses 1 core only, slower but consumes less CPU\n",
    "        grid_search = GridSearchCV(all_pipe, search_space, cv=5, verbose=1, refit = 'neg_mean_squared_error', scoring = scorer, return_train_score = True, n_jobs = -1)\n",
    "        #scoring option is for defining multiple scorers, otherwise null is OK\n",
    "        #refit must be chosen if multi scorers is selected. But best_score_, best_params_ etc will give only the result of that metric, cant get multi scores\n",
    "        #here REFIT option tells you which metric do you want to consider for best_params_ calculation(according to which metric)\n",
    "        \n",
    "        grid_search.fit(df_x, df_y.values.ravel())\n",
    "        \n",
    "        #from the resulting parameters dictionary, we choose the index of best_param_ set. This index will help us the get best param value from subresults\n",
    "        #metric results are in arraf form, in each array there is a list of results corresponding to the all parameter combinations. \n",
    "        ind = grid_search.best_index_      \n",
    "        \n",
    "        print(\"model = {}\".format(key))\n",
    "        print(\"train_mse = {}, test_mse ={}\".format(grid_search.cv_results_['mean_train_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind]))\n",
    "        print(\"train_mae = {}, test_mae ={}\".format(grid_search.cv_results_['mean_train_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind]))\n",
    "        print(\"train_r2 = {}, test_r2 = {}\".format(grid_search.cv_results_['mean_train_r2'][ind], grid_search.cv_results_['mean_test_r2'][ind]))\n",
    "        print(\"avg_fit_time = {}\".format(grid_search.cv_results_['mean_fit_time'][ind]))\n",
    "        #we are getting best_params_ from grid_search object still, it is valid\n",
    "        print(\"best_params = {}\".format(grid_search.best_params_))\n",
    "        print(\"---%0.1f minutes---\" %((timeit.default_timer()-start_time)/60))\n",
    "        print(\"...{}...\".format(start_time))\n",
    "        print(\"current_time = {}...\".format(datetime.now()))\n",
    "        print(\"current_time = {}...\".format(datetime.now()))\n",
    "        print(\"-----------------------------------\")\n",
    "        \n",
    "        best_params = dict()\n",
    "        best_params[key] = grid_search.best_params_\n",
    "        \n",
    "        best_results = dict()\n",
    "        best_results[key] = [grid_search.cv_results_['mean_test_neg_mean_squared_error'][ind], grid_search.cv_results_['mean_test_neg_mean_absolute_error'][ind], grid_search.cv_results_['mean_test_r2'][ind]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below part is for finalizing the selected regression model with whole data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x0^2', 'x0 x1', 'x0 x2', 'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6', 'x0 x7', 'x0 x8', 'x0 x9', 'x0 x10', 'x0 x11', 'x0 x12', 'x1^2', 'x1 x2', 'x1 x3', 'x1 x4', 'x1 x5', 'x1 x6', 'x1 x7', 'x1 x8', 'x1 x9', 'x1 x10', 'x1 x11', 'x1 x12', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5', 'x2 x6', 'x2 x7', 'x2 x8', 'x2 x9', 'x2 x10', 'x2 x11', 'x2 x12', 'x3^2', 'x3 x4', 'x3 x5', 'x3 x6', 'x3 x7', 'x3 x8', 'x3 x9', 'x3 x10', 'x3 x11', 'x3 x12', 'x4^2', 'x4 x5', 'x4 x6', 'x4 x7', 'x4 x8', 'x4 x9', 'x4 x10', 'x4 x11', 'x4 x12', 'x5^2', 'x5 x6', 'x5 x7', 'x5 x8', 'x5 x9', 'x5 x10', 'x5 x11', 'x5 x12', 'x6^2', 'x6 x7', 'x6 x8', 'x6 x9', 'x6 x10', 'x6 x11', 'x6 x12', 'x7^2', 'x7 x8', 'x7 x9', 'x7 x10', 'x7 x11', 'x7 x12', 'x8^2', 'x8 x9', 'x8 x10', 'x8 x11', 'x8 x12', 'x9^2', 'x9 x10', 'x9 x11', 'x9 x12', 'x10^2', 'x10 x11', 'x10 x12', 'x11^2', 'x11 x12', 'x12^2', 'x0^3', 'x0^2 x1', 'x0^2 x2', 'x0^2 x3', 'x0^2 x4', 'x0^2 x5', 'x0^2 x6', 'x0^2 x7', 'x0^2 x8', 'x0^2 x9', 'x0^2 x10', 'x0^2 x11', 'x0^2 x12', 'x0 x1^2', 'x0 x1 x2', 'x0 x1 x3', 'x0 x1 x4', 'x0 x1 x5', 'x0 x1 x6', 'x0 x1 x7', 'x0 x1 x8', 'x0 x1 x9', 'x0 x1 x10', 'x0 x1 x11', 'x0 x1 x12', 'x0 x2^2', 'x0 x2 x3', 'x0 x2 x4', 'x0 x2 x5', 'x0 x2 x6', 'x0 x2 x7', 'x0 x2 x8', 'x0 x2 x9', 'x0 x2 x10', 'x0 x2 x11', 'x0 x2 x12', 'x0 x3^2', 'x0 x3 x4', 'x0 x3 x5', 'x0 x3 x6', 'x0 x3 x7', 'x0 x3 x8', 'x0 x3 x9', 'x0 x3 x10', 'x0 x3 x11', 'x0 x3 x12', 'x0 x4^2', 'x0 x4 x5', 'x0 x4 x6', 'x0 x4 x7', 'x0 x4 x8', 'x0 x4 x9', 'x0 x4 x10', 'x0 x4 x11', 'x0 x4 x12', 'x0 x5^2', 'x0 x5 x6', 'x0 x5 x7', 'x0 x5 x8', 'x0 x5 x9', 'x0 x5 x10', 'x0 x5 x11', 'x0 x5 x12', 'x0 x6^2', 'x0 x6 x7', 'x0 x6 x8', 'x0 x6 x9', 'x0 x6 x10', 'x0 x6 x11', 'x0 x6 x12', 'x0 x7^2', 'x0 x7 x8', 'x0 x7 x9', 'x0 x7 x10', 'x0 x7 x11', 'x0 x7 x12', 'x0 x8^2', 'x0 x8 x9', 'x0 x8 x10', 'x0 x8 x11', 'x0 x8 x12', 'x0 x9^2', 'x0 x9 x10', 'x0 x9 x11', 'x0 x9 x12', 'x0 x10^2', 'x0 x10 x11', 'x0 x10 x12', 'x0 x11^2', 'x0 x11 x12', 'x0 x12^2', 'x1^3', 'x1^2 x2', 'x1^2 x3', 'x1^2 x4', 'x1^2 x5', 'x1^2 x6', 'x1^2 x7', 'x1^2 x8', 'x1^2 x9', 'x1^2 x10', 'x1^2 x11', 'x1^2 x12', 'x1 x2^2', 'x1 x2 x3', 'x1 x2 x4', 'x1 x2 x5', 'x1 x2 x6', 'x1 x2 x7', 'x1 x2 x8', 'x1 x2 x9', 'x1 x2 x10', 'x1 x2 x11', 'x1 x2 x12', 'x1 x3^2', 'x1 x3 x4', 'x1 x3 x5', 'x1 x3 x6', 'x1 x3 x7', 'x1 x3 x8', 'x1 x3 x9', 'x1 x3 x10', 'x1 x3 x11', 'x1 x3 x12', 'x1 x4^2', 'x1 x4 x5', 'x1 x4 x6', 'x1 x4 x7', 'x1 x4 x8', 'x1 x4 x9', 'x1 x4 x10', 'x1 x4 x11', 'x1 x4 x12', 'x1 x5^2', 'x1 x5 x6', 'x1 x5 x7', 'x1 x5 x8', 'x1 x5 x9', 'x1 x5 x10', 'x1 x5 x11', 'x1 x5 x12', 'x1 x6^2', 'x1 x6 x7', 'x1 x6 x8', 'x1 x6 x9', 'x1 x6 x10', 'x1 x6 x11', 'x1 x6 x12', 'x1 x7^2', 'x1 x7 x8', 'x1 x7 x9', 'x1 x7 x10', 'x1 x7 x11', 'x1 x7 x12', 'x1 x8^2', 'x1 x8 x9', 'x1 x8 x10', 'x1 x8 x11', 'x1 x8 x12', 'x1 x9^2', 'x1 x9 x10', 'x1 x9 x11', 'x1 x9 x12', 'x1 x10^2', 'x1 x10 x11', 'x1 x10 x12', 'x1 x11^2', 'x1 x11 x12', 'x1 x12^2', 'x2^3', 'x2^2 x3', 'x2^2 x4', 'x2^2 x5', 'x2^2 x6', 'x2^2 x7', 'x2^2 x8', 'x2^2 x9', 'x2^2 x10', 'x2^2 x11', 'x2^2 x12', 'x2 x3^2', 'x2 x3 x4', 'x2 x3 x5', 'x2 x3 x6', 'x2 x3 x7', 'x2 x3 x8', 'x2 x3 x9', 'x2 x3 x10', 'x2 x3 x11', 'x2 x3 x12', 'x2 x4^2', 'x2 x4 x5', 'x2 x4 x6', 'x2 x4 x7', 'x2 x4 x8', 'x2 x4 x9', 'x2 x4 x10', 'x2 x4 x11', 'x2 x4 x12', 'x2 x5^2', 'x2 x5 x6', 'x2 x5 x7', 'x2 x5 x8', 'x2 x5 x9', 'x2 x5 x10', 'x2 x5 x11', 'x2 x5 x12', 'x2 x6^2', 'x2 x6 x7', 'x2 x6 x8', 'x2 x6 x9', 'x2 x6 x10', 'x2 x6 x11', 'x2 x6 x12', 'x2 x7^2', 'x2 x7 x8', 'x2 x7 x9', 'x2 x7 x10', 'x2 x7 x11', 'x2 x7 x12', 'x2 x8^2', 'x2 x8 x9', 'x2 x8 x10', 'x2 x8 x11', 'x2 x8 x12', 'x2 x9^2', 'x2 x9 x10', 'x2 x9 x11', 'x2 x9 x12', 'x2 x10^2', 'x2 x10 x11', 'x2 x10 x12', 'x2 x11^2', 'x2 x11 x12', 'x2 x12^2', 'x3^3', 'x3^2 x4', 'x3^2 x5', 'x3^2 x6', 'x3^2 x7', 'x3^2 x8', 'x3^2 x9', 'x3^2 x10', 'x3^2 x11', 'x3^2 x12', 'x3 x4^2', 'x3 x4 x5', 'x3 x4 x6', 'x3 x4 x7', 'x3 x4 x8', 'x3 x4 x9', 'x3 x4 x10', 'x3 x4 x11', 'x3 x4 x12', 'x3 x5^2', 'x3 x5 x6', 'x3 x5 x7', 'x3 x5 x8', 'x3 x5 x9', 'x3 x5 x10', 'x3 x5 x11', 'x3 x5 x12', 'x3 x6^2', 'x3 x6 x7', 'x3 x6 x8', 'x3 x6 x9', 'x3 x6 x10', 'x3 x6 x11', 'x3 x6 x12', 'x3 x7^2', 'x3 x7 x8', 'x3 x7 x9', 'x3 x7 x10', 'x3 x7 x11', 'x3 x7 x12', 'x3 x8^2', 'x3 x8 x9', 'x3 x8 x10', 'x3 x8 x11', 'x3 x8 x12', 'x3 x9^2', 'x3 x9 x10', 'x3 x9 x11', 'x3 x9 x12', 'x3 x10^2', 'x3 x10 x11', 'x3 x10 x12', 'x3 x11^2', 'x3 x11 x12', 'x3 x12^2', 'x4^3', 'x4^2 x5', 'x4^2 x6', 'x4^2 x7', 'x4^2 x8', 'x4^2 x9', 'x4^2 x10', 'x4^2 x11', 'x4^2 x12', 'x4 x5^2', 'x4 x5 x6', 'x4 x5 x7', 'x4 x5 x8', 'x4 x5 x9', 'x4 x5 x10', 'x4 x5 x11', 'x4 x5 x12', 'x4 x6^2', 'x4 x6 x7', 'x4 x6 x8', 'x4 x6 x9', 'x4 x6 x10', 'x4 x6 x11', 'x4 x6 x12', 'x4 x7^2', 'x4 x7 x8', 'x4 x7 x9', 'x4 x7 x10', 'x4 x7 x11', 'x4 x7 x12', 'x4 x8^2', 'x4 x8 x9', 'x4 x8 x10', 'x4 x8 x11', 'x4 x8 x12', 'x4 x9^2', 'x4 x9 x10', 'x4 x9 x11', 'x4 x9 x12', 'x4 x10^2', 'x4 x10 x11', 'x4 x10 x12', 'x4 x11^2', 'x4 x11 x12', 'x4 x12^2', 'x5^3', 'x5^2 x6', 'x5^2 x7', 'x5^2 x8', 'x5^2 x9', 'x5^2 x10', 'x5^2 x11', 'x5^2 x12', 'x5 x6^2', 'x5 x6 x7', 'x5 x6 x8', 'x5 x6 x9', 'x5 x6 x10', 'x5 x6 x11', 'x5 x6 x12', 'x5 x7^2', 'x5 x7 x8', 'x5 x7 x9', 'x5 x7 x10', 'x5 x7 x11', 'x5 x7 x12', 'x5 x8^2', 'x5 x8 x9', 'x5 x8 x10', 'x5 x8 x11', 'x5 x8 x12', 'x5 x9^2', 'x5 x9 x10', 'x5 x9 x11', 'x5 x9 x12', 'x5 x10^2', 'x5 x10 x11', 'x5 x10 x12', 'x5 x11^2', 'x5 x11 x12', 'x5 x12^2', 'x6^3', 'x6^2 x7', 'x6^2 x8', 'x6^2 x9', 'x6^2 x10', 'x6^2 x11', 'x6^2 x12', 'x6 x7^2', 'x6 x7 x8', 'x6 x7 x9', 'x6 x7 x10', 'x6 x7 x11', 'x6 x7 x12', 'x6 x8^2', 'x6 x8 x9', 'x6 x8 x10', 'x6 x8 x11', 'x6 x8 x12', 'x6 x9^2', 'x6 x9 x10', 'x6 x9 x11', 'x6 x9 x12', 'x6 x10^2', 'x6 x10 x11', 'x6 x10 x12', 'x6 x11^2', 'x6 x11 x12', 'x6 x12^2', 'x7^3', 'x7^2 x8', 'x7^2 x9', 'x7^2 x10', 'x7^2 x11', 'x7^2 x12', 'x7 x8^2', 'x7 x8 x9', 'x7 x8 x10', 'x7 x8 x11', 'x7 x8 x12', 'x7 x9^2', 'x7 x9 x10', 'x7 x9 x11', 'x7 x9 x12', 'x7 x10^2', 'x7 x10 x11', 'x7 x10 x12', 'x7 x11^2', 'x7 x11 x12', 'x7 x12^2', 'x8^3', 'x8^2 x9', 'x8^2 x10', 'x8^2 x11', 'x8^2 x12', 'x8 x9^2', 'x8 x9 x10', 'x8 x9 x11', 'x8 x9 x12', 'x8 x10^2', 'x8 x10 x11', 'x8 x10 x12', 'x8 x11^2', 'x8 x11 x12', 'x8 x12^2', 'x9^3', 'x9^2 x10', 'x9^2 x11', 'x9^2 x12', 'x9 x10^2', 'x9 x10 x11', 'x9 x10 x12', 'x9 x11^2', 'x9 x11 x12', 'x9 x12^2', 'x10^3', 'x10^2 x11', 'x10^2 x12', 'x10 x11^2', 'x10 x11 x12', 'x10 x12^2', 'x11^3', 'x11^2 x12', 'x11 x12^2', 'x12^3']\n",
      "['AVM_AVM' 'AVM_NAVM' 'ARRIVAL_HOUR_10' 'ARRIVAL_HOUR_11'\n",
      " 'ARRIVAL_HOUR_12' 'ARRIVAL_HOUR_13' 'ARRIVAL_HOUR_14' 'ARRIVAL_HOUR_15'\n",
      " 'ARRIVAL_HOUR_16' 'ARRIVAL_HOUR_8' 'ARRIVAL_HOUR_9' 'ARRIVAL_HOUR_oth']\n",
      "[ 0.00000000e+00  1.40932373e+01  8.63408678e-02 -0.00000000e+00\n",
      " -0.00000000e+00  1.85192777e-02 -0.00000000e+00 -1.16296772e-03\n",
      " -1.85540882e-03 -0.00000000e+00 -9.20631802e-02  4.87204616e-03\n",
      "  6.94234236e-02 -0.00000000e+00 -3.39687036e+01  8.84028905e+00\n",
      "  8.03798847e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  2.27902403e-02  1.15071646e-01  0.00000000e+00\n",
      " -3.76600045e-02 -0.00000000e+00 -0.00000000e+00  4.37162503e-02\n",
      "  0.00000000e+00  4.66392837e-02  1.30498164e-01  7.48681806e-02\n",
      " -1.33995988e-02 -0.00000000e+00 -2.07693308e-01 -7.32926296e-03\n",
      "  1.86683526e-01  8.60411927e-02 -3.69940013e-02 -4.13540782e-03\n",
      " -3.53069376e-02  0.00000000e+00 -1.86292103e-02  0.00000000e+00\n",
      " -2.35496002e-02  5.78027475e-03 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  4.58453140e-02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.07362043e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -2.21242764e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -8.50354666e-02\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.24680981e-01\n",
      "  0.00000000e+00  0.00000000e+00  1.52088008e-01  0.00000000e+00\n",
      " -0.00000000e+00  3.57615580e+01 -1.82802096e+01 -2.69676328e+01\n",
      " -5.57476511e+00 -5.16910795e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  1.72816912e+00\n",
      "  4.51322550e-01 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -5.71849224e-01\n",
      " -6.76408044e-01 -0.00000000e+00  4.02341187e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  5.96600404e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  5.24664582e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -2.03321214e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.95432617e+00  0.00000000e+00 -0.00000000e+00\n",
      "  1.27681707e-01  0.00000000e+00  1.00776531e-01  1.12572963e-01\n",
      "  5.24949760e-03 -0.00000000e+00  0.00000000e+00 -4.17720844e-02\n",
      " -0.00000000e+00  1.81880825e-01  1.82173016e-01 -4.26630204e-02\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.45062329e-02\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.29010395e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.49763566e-03\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.05167245e-02  0.00000000e+00  0.00000000e+00  3.35858242e-03\n",
      "  0.00000000e+00 -5.77695011e-04 -9.81690733e-04 -2.01366928e-03\n",
      " -0.00000000e+00 -1.53724657e-03  0.00000000e+00 -0.00000000e+00\n",
      "  2.33851564e-03 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -2.56680598e-06  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -9.45062731e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.02607602e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -3.92678777e-03  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.60039338e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -3.93092881e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.41531991e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.18904813e-01  0.00000000e+00  0.00000000e+00 -0.00000000e+00]\n",
      "4.668210477286275\n",
      "           DESI   AVM ARRIVAL_HOUR    real         pred\n",
      "21302    78.995   AVM           16    79.0   141.951097\n",
      "20956  2139.995  NAVM            8  3000.0  2329.675634\n",
      "22845   424.004   AVM           11   679.0   611.841110\n",
      "17510    79.000   AVM           11   130.0   231.974779\n",
      "16387    34.908  NAVM           16    73.0    96.522385\n"
     ]
    }
   ],
   "source": [
    "if finalize == 1:\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lasso = Lasso(alpha = 0.0001, max_iter = 500.0, tol = 0.0001)\n",
    "    lgb = LGBMRegressor(learning_rate= 0.1, max_depth= 18, n_estimators= 100, num_leaves= 51)\n",
    "    \"\"\"\n",
    "    numeric_pipe = make_pipeline(MinMaxScaler(feature_range = (0,1)))\n",
    "    categoric_pipe = make_pipeline(OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "    preprocessor = ColumnTransformer(transformers = [('num',numeric_pipe, num_cols), ('cat',categoric_pipe,cat_cols)])\n",
    "    \n",
    "    regr_pipe_final = make_pipeline(preprocessor, lr)\n",
    "    \n",
    "    regr_pipe_final.fit(df_x, df_y.values.ravel())\n",
    "    \n",
    "    print(regr_pipe_final.steps[0][1][1])\n",
    "    print(regr_pipe_final.steps[1][1].coef_)\n",
    "    print(regr_pipe_final.steps[1][1].intercept_)\n",
    "    \"\"\"\n",
    "    \n",
    "    #numeric_pipe = make_pipe('minmax', MinMaxScaler(feature_range = (0,1)))\n",
    "    #categoric_pipe = Pipeline('onehot', OneHotEncoder(sparse = True, handle_unknown='ignore'))\n",
    "    poly = PolynomialFeatures(3)\n",
    "    preprocessor = ColumnTransformer(transformers = [('num', MinMaxScaler(feature_range = (0,1)), num_cols), \n",
    "                                                     ('cat', OneHotEncoder(sparse = True, handle_unknown='ignore'), cat_cols)])\n",
    "    regr_pipe_final = Pipeline([('prep', preprocessor), ('pol', poly), ('regr',lasso)])\n",
    "    \n",
    "    regr_pipe_final.fit(df_x, df_y.values.ravel())\n",
    "    \n",
    "    # pol is coming after preprocessor in pipeline. So, it is using the output variables of preprocessor as input.\n",
    "    # you can see the polynomial features are increased to 84, which are one hot encoded features mostly. \n",
    "    #So, model is automatically upgraidng itself after each step inside pipeline\n",
    "    print(regr_pipe_final.named_steps['pol'].get_feature_names())\n",
    "    print(regr_pipe_final.named_steps['prep'].transformers_[1][1].get_feature_names(cat_cols))\n",
    "    print(regr_pipe_final.named_steps['regr'].coef_)\n",
    "    print(regr_pipe_final.named_steps['regr'].intercept_)\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"polynomial_features = {}\".format(regr_pipe_final.named_steps['pol'].get_feature_names()), file = text_file2)\n",
    "    print(\"one_hot_features = {}\".format(regr_pipe_final.named_steps['prep'].transformers_[1][1].get_feature_names(cat_cols)), file = text_file2)\n",
    "    print(\"regr_coefficients = {}\".format(regr_pipe_final.named_steps['regr'].coef_), file = text_file2)\n",
    "    print(\"regr_intercept = {}\".format(regr_pipe_final.named_steps['regr'].intercept_), file = text_file2)\n",
    "    \"\"\"\n",
    "    \n",
    "    var_dict = {}\n",
    "    var_list = []\n",
    "    var_list = num_cols + list(regr_pipe_final.named_steps['prep'].transformers_[1][1].get_feature_names(cat_cols))\n",
    "    \n",
    "    for i in range(len(var_list)):\n",
    "        var_dict['x'+str(i)] = var_list[i]\n",
    "        \n",
    "    effect_dict = {}\n",
    "    coeff_dict = {}\n",
    "    \n",
    "    pol_list = []\n",
    "    pol_list = regr_pipe_final.named_steps['pol'].get_feature_names(var_list)\n",
    "    \n",
    "    for i in range(1,len(regr_pipe_final.named_steps['pol'].get_feature_names())):\n",
    "        if regr_pipe_final.named_steps['regr'].coef_[i] != 0:\n",
    "            #coeff_dict[str(regr_pipe_final.named_steps['pol'].get_feature_names()[i])] = [round(regr_pipe_final.named_steps['regr'].coef_[i], 4), var_list[i-1]]\n",
    "            #effect_dict[var_list[i-1]] = round(regr_pipe_final.named_steps['regr'].coef_[i], 4)\n",
    "            coeff_dict[str(regr_pipe_final.named_steps['pol'].get_feature_names()[i])] = [round(regr_pipe_final.named_steps['regr'].coef_[i], 4), pol_list[i]]\n",
    "            effect_dict[pol_list[i]] = round(regr_pipe_final.named_steps['regr'].coef_[i], 4)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    effect_dict = dict(sorted(coeff_dict.items(), key=lambda x: x[1]))\n",
    "    \n",
    "    pred = regr_pipe_final.predict(df_x)\n",
    "    df_x['real'] = np.exp(df_y)\n",
    "    df_x['pred'] = np.exp(pred)\n",
    "    \n",
    "    #df_x['real'] = pow(df_y, 2)\n",
    "    #df_x['pred'] = pow(pred, 2)\n",
    "    print(df_x.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last part is for readability, and post analysis of variable coefficients. Also, it transforms the output into a more readable format by optimization algorithm. Also, it writes all of these into a text file as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           DESI   AVM ARRIVAL_HOUR    real         pred\n",
      "21302    78.995   AVM           16    79.0   141.951097\n",
      "20956  2139.995  NAVM            8  3000.0  2329.675634\n",
      "22845   424.004   AVM           11   679.0   611.841110\n",
      "17510    79.000   AVM           11   130.0   231.974779\n",
      "16387    34.908  NAVM           16    73.0    96.522385\n"
     ]
    }
   ],
   "source": [
    "    #df_x['real'] = pow(df_y, 2)\n",
    "    #df_x['pred'] = pow(pred, 2)\n",
    "    print(df_x.head(5))\n",
    "    \n",
    "    dictcount = 0\n",
    "    finalstr = str()\n",
    "    for key, value in effect_dict.items():\n",
    "        dictcount += 1\n",
    "        substr = value[1].split()\n",
    "        substr2 = str()\n",
    "        for i in range(len(substr)):\n",
    "            if '^' in substr[i]:\n",
    "                substr[i] = substr[i].replace('^', ' ')\n",
    "                if i == len(substr) - 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    substr[i] = str(substr[i] + ' ')\n",
    "            else:\n",
    "                if i == len(substr) - 1:\n",
    "                    substr[i] = substr[i] + ' 1'\n",
    "                else:\n",
    "                    substr[i] = str(substr[i] + ' 1 ')\n",
    "            substr2 = substr2 + substr[i]\n",
    "        if dictcount != len(effect_dict.items()):\n",
    "            finalstr = finalstr + str(value[0]) + ' ' + substr2 + ','\n",
    "        else:\n",
    "            finalstr = finalstr + str(value[0]) + ' ' + substr2\n",
    "\n",
    "    finalstr = str(round(regr_pipe_final.named_steps['regr'].intercept_, 4)) + ' INTERCEPT 0,' + finalstr\n",
    "    \n",
    "    print(\"features = {}\".format(var_dict), file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"regr_coefficients = {}\".format(coeff_dict), file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"regr_effects = {}\".format(effect_dict), file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"regr_intercept = {}\".format(regr_pipe_final.named_steps['regr'].intercept_), file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"------------------------------------\", file = text_file2)\n",
    "    print(\"parsed_coefficients = {}\".format(finalstr), file = text_file2)\n",
    "    #print_excel = df_x.to_excel(r'C:\\Users\\ali.kilinc\\Desktop\\PredNikeLasso20filteredLastDA.xlsx', index = None, header = True, sheet_name = 'FullTable')\n",
    "    \n",
    "\n",
    "text_file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
